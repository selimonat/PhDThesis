
Multisensory integration makes ecological sense when the incoming signals
refer to the same external entity or, more generally, are due to the same
underlying physical events. In general, there are three different ways in
which signals must correspond for integration to take place: spatially,
temporally, and/or semantically \citep{stein1993a, macaluso2005a,
calvert2001c, doehrmann2008a}.



These correspondences represent a default scheme of integration, and in
special cases are influential enough to fool the system into binding
streams that do not belong to the same underlying cause. In the well-known
ventriloquist illusion, for instance, the concurrent temporal modulation of
the puppet's mouth and the ventriloquist's speech leads to a perception of
a talking puppet. In spite of the spatial and even semantic discrepancies
here, temporal correspondence drives this audiovisual integration
\citep{vroomen2004a, bonath2007a}. A competition in temporal synchronicity
between auditory and visual streams, on the other hand, may lead to a
visual illusion. In the case of the sound-induced flash illusion
\citep{shams2000a, shams2002a, mishra2007a, mishra2008a} an extra illusory
flash is perceived when a single flash is interleaved between two beeps.
More generally, at a constant visual stimulation rate, the number of
perceived flashes increases with a higher auditory stimulation rate (visual
illusion) and decreases when the rate of auditory events is slower (visual
suppression) \citep[e.g.][]{shipley1964a, noesselt2008a}.  The perceived
temporal pattern of visual events is thus "adjusted" to match the rate of
auditory events. Taken together, these cases nicely illustrate that
temporal aspects are essential to the perception of audiovisual events.



Research into the neural substrate of multisensory integration began in
earnest after neurophysiological results showed characteristic multimodal
response profiles at the level of single cells in superior colliculus in
cat \citep{meredith1983a, meredith1987a} and macaque \citep{wallace1996a}.
When stimuli were presented in close temporal proximity in two sensory
modalities, the response of some collicular cells was found to reach or
even exceed the sum of responses to each stimulus delivered in isolation.
Imaging studies have since confirmed the importance of superior colliculus
in human audiovisual integration \citep{calvert2001b, miller2005a}.
Several cortical areas have also been implicated in the processing of
audiovisual stimuli including the insula \citep{bushara2001a} and
intraparietal sulcus \citep{calvert2001c}. The superior temporal sulcus, in
particular, has been consistently shown to be an area of audiovisual
convergence and integration (\cite{beauchamp2004a}, see \cite{calvert2001b}
for a review of earlier work).  Importantly, it has been found to be
sensitive to the temporal synchrony of audiovisual information, especially
audiovisual speech \citep{calvert2000a, miller2005a, macaluso2004a}, but
also simpler stimuli \citep{noesselt2007a, atteveldt2007a}.  In addition,
there is a growing body of evidence from ERP \nomenclature{ERP}{Event
Related Potential} and fMRI \nomenclature{fMRI}{Functional Magnetic
Resonance Imaging} studies demonstrating that areas traditionally conceived
to be unisensory also play a role in the synthesis of audiovisual
information (\cite{giard1999a, molholm2002a, kayser2007a, calvert1999a,
miller2005a, noesselt2007a}, for general reviews on low-level integration
see \cite{foxe2005a, kayser2007b, driver2008a}). Another line of research
follows the hypothesis that cortically, multisensory integration is
instantiated by the relative timing of synchronized populations of cortical
neurons. This has been supported by human EEG/MEG
\nomenclature{MEG}{Magnetic Encephalogram} studies (see \cite{
senkowski2008a} for a review). 



To date, studies that explicitly investigate the temporal dependence of
audiovisual binding, such as those mentioned above, have mainly manipulated
the timing of stimulus onset or coincidence in one modality relative to the
other. Typically, these experiments employ brief, simple stimuli that are
either presented individually \citep{meredith1987a, bushara2001a} or in
streams \citep{calvert2001c, noesselt2007a, dhamala2007a, senkowski2007a}.
However, such static audiovisual events are rarely found under natural
conditions, where acoustic signals mostly emanate continuously from objects
in motion, such as rustling leaves or moving cars. Real-world audiovisual
events extend over time and are bound less by their simultaneity and more
by their common temporal dynamics. For this reason, concentrating only on
the isolated temporal coincidence of brief events neglects important
temporal aspects inherent in natural events. To capture the full importance
of temporal information, it is thus necessary to investigate audiovisual
events extended in time. 



Indeed, Kayser and K\"onig \citep{kayser2004b} have recently demonstrated
that extended stimulation with natural visual stimuli leads to a continuous
modulation of LFP power in cat visual cortex, and that this modulation
reflects the dynamics of the presented movies. In addition, rhythmic
auditory stimulation has been shown to entrain low frequency activity in
auditory cortex of macaque \citep{lakatos2005a}. The entrainment of human
cortical activity to stimulation at a constant frequency is also a
well-known phenomenon in the auditory \citep{galambos1981a,
bidet-caulet2007a} and visual modalities \citep[e.g.][]{regan1966a,
ding2006a}, however it is not clear whether this also holds for stimuli
with rich temporal power spectra. One possible role of stimulus locking in
temporally-dependent multisensory integration may involve an increase in
the efficacy of stimuli from a second modality when temporally aligned with
neural activity that is entrained to the first modality
\citep{lakatos2007a, kayser2008a, schroeder2008a}. As such, the extended
changes in cortical activity seen in response to extended sensory
stimulation are certainly worth closer examination in the context of
audiovisual integration.



Here, we are interested in the general relevance of extended, time-varying
information for the integration of visual and auditory streams. For this
purpose, we paired well-defined visual and auditory signals that changed
continuously and irregularly over time according to shared or differing
dynamical patterns. Though relatively complex in temporal structure, and in
this respect comparable to natural events including audiovisual speech, our
stimuli have the advantage of being novel to the experimental subject and
free of semantic reference. Using EEG, we employ two measures of stimulus
locking to investigate audiovisual interaction. First, as a generalization
of the evoked potential, we examine whether averaged EEG waveforms are
locked to stimulus dynamics. Evoked potentials reveal the activity locked
to stimulus onset, and here we extend this approach using
cross-correlation. Second, following \cite{kayser2004b} study, we
investigate the locking of EEG power dynamics at frequencies within and
beyond the range of our stimulus dynamics, and thus whether the
observations made in cat transfer to man. 



We specifically address three research questions. First, whether neural
activity in the human cortex locks to the temporal structure of irregular
dynamic stimuli in the visual and auditory modalities. Second and most
importantly, we want to investigate whether multisensory integration is
reflected in such a mechanism. Finally, we are interested in the time
course of stimulus-locking, particularly in response to congruent and
incongruent bimodal stimulation.


