
\subsection{Participants and recording}

Forty-two subjects (19 males, mean age 23) participated in the experiment.
All subjects gave informed written consent but were naive to the purpose of
the experiment. All experimental procedures were in compliance with
guidelines described in Declaration of Helsinki. Each subject performed
only one session.

The experiments were conducted in a small dimly lit room. The subjects sat
in a stable chair with back support, facing a monitor. A video-based
head-mounted eye tracker (Eye Link 2, SR Research, Ontario, Canada) with
sampling rate of 250 Hz and nominal spatial resolution of $0.01^\circ$ was
used for recording eye movements.

For calibration purposes, subjects were asked to fixate points appearing in
a random sequence on a $3 \times 3$ grid by using built-in programs
provided with Eye Link (similar to \cite{tatler2006a}). This procedure was
repeated several times to obtain optimal accuracy of calibration. It lasted
for several minutes, thus allowing subjects to adapt to the conditions in
the experimental room. All the data analyzed in the present article were
obtained from recordings with an average absolute global error of less then
$0.3^\circ$.

During the experiment, a fixation point appeared in the center of the
screen before each stimulus presentation. The experimenter triggered the
stimulus display only after the subject had fixated this point. The data
obtained during this control fixation were used to correct for slow drifts
of the eye tracker; that is, if drift errors were high a new calibration
protocol was started again. Subjects could take a break and remove the
headset at any time. In those instances, which occurred rarely, the
continuation of the experiment began with the calibration procedure
described above.

\subsection{Stimuli}

The multimodal stimuli consisted of images and sounds. Images depicted
natural scenes like forests, bushes, branches, hills, open landscapes,
close ups of grasses and stones, but also to a limited extent human
artifacts, for example, roads, house parts. Some of these stimuli were used
in a previous study \citep{einhaeuser2003a}. The photographs were taken
using a 3.3 megapixel digital camera (Nikon Coolpix 995, Tokyo, Japan),
down-sampled to a resolution of 1024 \texttimes 768 pixels, and converted
to grayscale. The images were displayed on a 21-in. CRT monitor (SyncMaster
1100DF, Samsung Electronics, Suwon, South Korea), at a resolution of 1024
\texttimes 768 pixels and with a refresh rate of 120 Hz. The distance of
the monitor from the subject's eyes was 80 cm. The stimuli covered
$28^\circ \times 21^\circ$ of visual angle on the horizontal and vertical
axes, respectively. The screen was calibrated for optimal contrast and
brightness using a commercial colormeter (EyeOne Display, GretagMacBeth,
Regensburg, Switzerland). The natural auditory stimuli were taken from free
samples of a commercial Internet Source (Askland Technologies Inc.,
Victorville, Canada). Overall, 32 different sound tracks were used. All of
these were songs of different birds, thus in accordance with the semantic
content of the images presented. The auditory stimuli were generated using
a sound card (Sound Blaster Audigy EX ZE Platinium Pro, Creative Labs,
Singapore) and loudspeakers (Logitech 2.1 Z-3, CA, USA). Loudspeakers
flanked both sides of the monitor at a distance of 20 cm, at the same depth
plane as the screen. In order to avoid the speakers attracting the
attention of the subjects, they were hidden behind black curtains. Sounds
were played from the left or right speaker, depending on the experimental
condition. The auditory signal amplitude was in the range of 50-70 and
55-75 dB for left and right conditions, respectively. The slight increase
was due to the acoustic conditions in the experimental room.

\subsection{Experimental paradigm}

We employed two unimodal conditions (visual and auditory) and one
multimodal condition (audiovisual) (\ref{fig1}A). During the visual
condition (V), 32 natural images were presented. The auditory condition
used $16 \times 2$ presentations of natural sounds, originating from left
(AL) and right (AR) side relative to the subject's visual field.

\begin{figure}[!htbp] \centerline{
\includegraphics[width=\textwidth]{./part_cm/figures/figure01.png}} 

\caption[Experimental Paradigm and Subject Selection.]{
\textbf{Experimental Paradigm and Subject Selection.} \textbf{(A)} Five
different experimental conditions are used to analyze the effect of
unimodal (visual and auditory) and crossmodal (audiovisual) stimuli on
overt behavior. 42 subjects studied 32 natural images in three different
conditions. In the visual condition (V), natural images were presented. In
multimodal conditions, the images were paired with localized natural sounds
originating from either the left (AVL) or right (AVR) side of the monitor.
In the remaining conditions (AL and AR), the sole effect of the localized
auditory stimulus was characterized. The duration of stimulus presentation
in all conditions was 6 seconds. \textbf{(B)} Distribution of values of
each subject's $\sigma$, which characterizes the horizontal spread of the
pdf $p_{s,v}(x,y)$ of a given subject's at condition V. $p_{s,v}(x,y)$ from
two subjects are shown in the inset with corresponding horizontal marginal
distributions shown above. Arrows mark the histogram bins to which these
subjects belong. The vertical dashed line indicates the threshold $\sigma$
value required for a given subject to be included in further analysis.}
\label{fig1}
\end{figure}


During auditory conditions, we presented auditory stimuli jointly with
white noise images. These were constructed by shuffling the pixel
coordinates of the original images. They lack any spatial structure and as
a result do not bias the fixation behavior of the subjects. Obtaining a
truly unimodal saliency map for auditory conditions adds some undesirable
technical issues. Firstly, due to the operation of the eye tracker and
monitor truly zero-light conditions are hard to achieve. Secondly,
presenting a dark stimulus leads to more fixations outside the dynamics
range of the monitor and eye tracker. Finally, a sudden drastic change in
mean luminance of the visual stimulus would introduce non-stationarities in
the form of dark adaptation and create a potential confound. To avoid these
problems, we presented white noise images of identical mean luminance as
the natural pictures.

The multimodal conditions (AVL and AVR) each comprised the simultaneous
presentation of 32 auditory and visual stimuli pairs, without any onset or
offset asynchrony. In order to balance the stimulus set, a new pairing of
audiovisual stimuli was presented on each side to each subject. Stimuli
were shown in a pseudorandom order, with a different permutation used for
each subject. Each stimulus was presented for 6 s. A given session
contained 128 stimulus presentations and lasted in total for up to 50 min.
The only instruction given to the subjects was to watch and listen
carefully to the images and sounds. No information about the presence of
the speakers at both sides of the monitor or the lateralization of the
auditory stimuli was provided to the subjects.

\subsection{Data analysis}

We defined fixation points and intervening saccades using a set of
heuristics. A saccade was characterized by an acceleration exceeding 8000
deg/s\textsuperscript{2}, a velocity above 30 deg/s, a motion threshold of
$0.1^\circ$, and a duration of more than 4 ms. The intervening episodes
were defined as fixation events. The result of applying these parameters
was plotted and was visually assessed to check that they produce reasonable
results.

\subsection{Probability distributions}

From the fixation points of the individual subjects, we built probability
density functions (pdf). The first fixation on each stimulus was discarded,
as it was always located at the position of the preceding fixation cross in
the center of the image. A pdf, $p_{s,i,c}(x, y)$, for a given subject
\textit{s}, image \textit{i}, and condition \textit{c} was calculated as in
equation \ref{cm_eq1}.


\begin{equation}
p_{s,i,c}(x,y,t)=\frac{1}{F}\sum_{f=1}^{F}\delta(x-x_f)\delta(y-y_f)\delta(t-t_f)
\label{cm_eq1} \end{equation} 




with $\delta(x)$ as the discrete Dirac function (the Dirac function is
equal to zero unless its argument is zero, and it has a unit integral).
$x_f$, $y_f$, and $t_f$ are the coordinates and time of the
f\textsuperscript{th} fixation. $F$ is the total number of fixations. We
distinguish three different pdfs for a given condition with respect to how
these individual pdfs were averaged: subject, image, and spatio-temporal
pdfs. Subject pdfs $p_{s,c}(x,y)$ for a given subject $s$ and condition $c$
were built by averaging all the pdfs obtained from a given subject over the
images, without mixing the conditions, according to \ref{cm_eq2}

\begin{equation} p_{s,c}(x, y)=\frac{1}{I}\sum_{i}^{I}p_{s,i,c}(x,y)
\label{cm_eq2} \end{equation} 


Image pdfs ($p(x, y)$) and spatio-temporal pdfs ($p(x, t)$) were similarly
computed by averaging over the appropriate dimensions. Image pdfs inform us
about consistent biases that influence subjects' gaze and are therefore
empirically determined saliency maps specific to a given image. Raw pdfs
are matrices of the same size as an image and store fixation counts in
their entries. In all cases, these raw pdfs were smoothed for further
analysis by convolution. A circular two-dimensional Gaussian kernel was
used, which had a unit integral and a width parameter $\sigma$ with a value
of $0.6^\circ$ unless otherwise stated. This spatial scale is twice as
large as the maximal calibration error and maintains sufficient spatial
structure for data analysis.

\subsection{PDF parameterization}


Several parameters were extracted from the pdfs,


\begin{equation} \mu_c = \frac{\sum_x{p_c(x)x }}{\sum_x{p_c(x)}} \label{cm_eq4} \end{equation} 


The center of gravity is measured according to Equation \ref{cm_eq4} in
order to quantify the global shift of subject and image pdfs. $\mu_c$ is
the center of gravity along the X-axis for condition $c$, $p_c(x)$ is the
marginal distribution of a pdf along the X-axis. In condition V, fixation
probabilities were usually distributed symmetrically on both sides of the
visual field with centralized center of gravity values. This simple
statistic successfully quantifies any bias toward the sound location,

\begin{equation} \sigma = [ \sum_x{(x-\mu_V)^2 p_V(x)}  ]^{1/2}  \label{cm_eq5} \end{equation} 

The spread of a given pdf was measured from a subject pdf under condition V
using Equation  \ref{cm_eq5} in order to quantify how explorative that
subject's scanning behavior was. $\sigma$ is the spread along the X-axis,
$p_V(x)$ is the marginal distribution along the X-axis of the subject pdf,
and $\mu_V$ is the center of gravity computed as in Equation \ref{cm_eq4}.
The marginal distributions arising from condition V were well-behaved
(\ref{fig1} inset), thus allowing us to examine the explorative behavior of
subjects. Seven of 42 subjects did not engage in explorative viewing
during the analysis of the images, resulting in small spread values. These
subjects were excluded from further analysis.


The spatio-temporal pdfs, $p_c(x, t)$, contain information about how the
fixation density along the horizontal axis varies over time. We assessed
the statistical differences of pdfs from different conditions in a
time-localized manner; that is, we obtained a p-value as a function of time
for three pairs of conditions (AVL and V; AVR and V; AL and AR). A
two-sided Kolmogorov-Smirnov goodness-of-fit hypothesis test in
corresponding temporal portions of the pdfs was used with significance
level $\alpha$ set to .001. This was done after binning the probability
distribution $p_c(x, t)$ over the time axis with a bin size of 240 ms,
yielding 25 time intervals for comparison per pair of conditions. A
temporal interval over which the null hypothesis was rejected in at least
in two of the condition pairs was considered as a temporal interval of
interest.

The similarity between image pdfs obtained from the same image under
different conditions \textemdash for example, $p_{i,V}(x, y)$ of image $i$
and condition V and $p_{i,AV}(x, y)$ of the same image and AV conditions
(i.e., either AVL or AVR) \textemdash was evaluated by computing
$r_{Vâˆ’AV}^2$. Before the coefficients were calculated, the AV image pdfs
were first normalized to $p_{i,AV}^N$ according to Equation \ref{cm_eq6}

\begin{equation} p_{i,AV}^{N} =
\frac{<p_V(x)>_i}{<p_{AV}(x)>_i}p_{i,AV}(x,y) \label{cm_eq6} \end{equation} 


This normalization corrects the image pdfs of a given bimodal condition for
the global effect of the sound location, so that the expected number of
fixations over the horizontal axis is the same over different conditions.
The resulting distribution of $r_{V,AV}^2$ values was then compared to a
control distribution of $r^2$ values, which measure the baseline
correlation between image pdfs coming from the same condition pairs (i.e.,
V and AVL, or V and AVR) but differing images. The Kullback-Leibler (KL)
divergence, which does not assume any a priori relationship between two
distributions, was used to quantify the similarity of the different image
pdfs obtained from different conditions. This measure was evaluated
according to the following formula \ref{cm_eq7}, where $D_{KL}(p_{i,c_1},
p_{i,c_2})$ denotes the KL divergence measure between two pdfs,
$p_{i,c_1}(x, y), p_{i,c_2} (x,y)$ in bits, 

\begin{equation} D_{KL}[p_{i,c_1}(x,y),p_{i,c_2}(x,y)] =
H[p_{i,c_1}(x,y),p_{i,c_2}(x,y)] -H[p_{i,c1}(x,y)] \label{cm_eq7}
\end{equation} 

\nomenclature{KL}{Kullback-Leibler}

The KL divergence measures the difference between the
cross-entropy of two probability distributions and the entropy of one of
them. The cross-entropy is always greater than or equal to the entropy;
therefore, the KL divergence is always greater than or equal
to zero, which allows its usage as a distance measure between two different
pdfs. However, unlike other distance measurements, it is not symmetric.
Therefore, it is used to measure the distance between the prior and
posterior distributions. In comparing different image pdfs, we used the
condition V as the prior distribution. One problem we encountered was zero
entries in the probability distributions. As the logarithm of zero is not
defined, a small constant ($c = 10^{-9}$) was added to all entries in the
pdf. The precise choice of this constant did not make a difference to the
results of our analysis.

\subsection{Modeling the cross-modal interaction}


In order to quantify the cross-modal interaction, we carried out a multiple
regression analysis. We devised a model (Equation \ref{cm_eq8}) with a
cross-product interaction term using smoothed unimodal and multimodal pdfs
as independent and dependent variables, respectively, 



\begin{equation} \label{cm_eq8}
p_{i,AV}=\beta_{i,0}+\beta_{i,1}\cdotp_{i,V}+\beta_{i,2} \cdot p_{i,A} +
\beta_{i,3} \cdot p_{i,V-A}  \end{equation} 

In Equation \ref{cm_eq8}, $p_{i,AV}$, $p_{i,V}$, and $p_{i,A}$ are the
image pdfs of image $i$ at audiovisual, visual, and auditory conditions,
respectively. The interaction term $p_{i,VA}$ is supposed to approximate
the image pdf that would arise from a multiplicative cross-modal
interaction. It is created by the element-wise multiplication of both
unimodal image pdfs and renormalized to a unit integral.


The integrative process was further characterized by constructing
integration plots. The probability of fixation at each x-y location was
extracted from the 32 image pdfs of visual, auditory, and bimodal
conditions, yielding a triplet of values representing the saliency. A given
triplet defines the saliency of a given image location in the multimodal
condition as a function of the saliency of the same location in both
unimodal conditions, represented by the point ($p_V(x, y), p_A(x, y),
p_{AV}(x, y)$) in the integration plot. These points were irregularly
distributed and filled the three-dimensional space unevenly. We discarded
the 15\% of the values which lay in sparsely distributed regions, and we
concentrated instead on the region where most of the data were located. The
data points inside this region of interest were then binned, yielding an
expected value and variance for each bin. Weighted least square analysis
was carried out to approximate the distribution by estimating the
coefficients of the following equation:

\begin{equation} \frac{p_{AV}}{g(p_{AV})} = \beta_0 + \beta_1 \cdot
\frac{p_V}{g(p_V)} + \beta_2 \cdot \frac{p_A}{g(p_A)} + \beta_3 \cdot
\frac{p_{V\cdot A}} {g(p_{V\cdot A})} \label{cm_eq9} \end{equation} 


The difference between the above equation and Equation \ref{cm_eq8} is that
Equation \ref{cm_eq9} does not take different images into consideration and
pools the data over images and space. Additionally, each individual
probability value is normalized by its geometric mean ($g(pc)$), which
normalizes for its individual range thus allowing a direct comparison of
the regression coefficients.


\subsection{Luminance contrast measurements}


LC was computed as the standard deviation of the luminance values of the
pixels inside a square patch of about $1^\circ$ centered at fixation
positions. The luminance contrast computed at the fixation points over
images and subjects yielded the actual contrast distribution. This
distribution was compared to a control distribution to evaluate a potential
bias at the fixation points. An unbiased pool of fixation coordinates
served as the control distribution \textemdash for a given image, this was
constructed by taking all fixations from all images other than the image
under consideration. This control distribution takes the center bias of the
subjects' fixations into account, as well as any potential systematic
effect in our stimulus database \citep{baddeley2006a, tatler2005a}. The
contrast effect at fixation points was computed by taking the ratio of the
average contrast values at control and actual fixations. In order to
evaluate the luminance contrast effect over time, the actual and control
fixation points were separated according to their occurrences in time. The
analysis was carried out using different temporal bin sizes ranging from
100 to 1000 ms. All analysis was carried out using MatLab (Mathworks,
Natick, MA, USA).

